{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from nltk.corpus import words\n",
    "import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = stopwords.words('english')\n",
    "common_words = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - GENE\n",
    "\n",
    "2 - DISEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../genes.txt\") as g:\n",
    "    genes = set([ x.lower().strip() for x in g.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.append(\"ace\")\n",
    "st.append(\"large\")\n",
    "st.append(\"kit\")\n",
    "st.append(\"impact\")\n",
    "st.append(\"set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16580/16580 [04:06<00:00, 67.32it/s]\n"
     ]
    }
   ],
   "source": [
    "genes_clean = []\n",
    "for g in tqdm(genes):\n",
    "    if g not in st and not g.isdigit() and g not in common_words:\n",
    "        genes_clean.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Maladies.txt\") as m:\n",
    "    maladies = list(set([ x.lower().strip() for x in m.readlines()]))\n",
    "    maladies.remove(\"disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = open(\"../asthma.json\").readlines()\n",
    "test = open(\"../autism.json/autism.json\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training dataset by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 44975/44975 [41:47<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_DATA_WORD=[]\n",
    "train_i = 0\n",
    "nb = 0\n",
    "for line in tqdm(datas):\n",
    "    text = json.loads(line)[\"ab\"]\n",
    "    train_i += 1\n",
    "    \n",
    "    detected_disease = []\n",
    "    for maladie in maladies:\n",
    "        if maladie in text:\n",
    "            detected_disease.append(maladie.lower())\n",
    "    \n",
    "    for sentence in tokenize.sent_tokenize(text):\n",
    "        if len(sentence) > 10:\n",
    "            uhm = {\"entities\": []}\n",
    "            \n",
    "            for detect in detected_disease:\n",
    "                if detect in sentence.lower():\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(detect), sentence.lower())]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(detect), \"DISEASE\")  )\n",
    "            \n",
    "            for n in tokenize.word_tokenize(sentence):\n",
    "                if n.lower() in genes_clean:\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n.lower()), sentence.lower())]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(n), \"GENE\")  )\n",
    "            \n",
    "            TRAIN_DATA_WORD.append( (sentence, uhm) )\n",
    "    \n",
    "    if train_i % 2000 == 0:\n",
    "        with open(\"train/normal/train_set_\"+str(nb)+\".json\", \"w\") as t:\n",
    "            t.write(json.dumps(TRAIN_DATA_WORD))\n",
    "        nb += 1\n",
    "        TRAIN_DATA_WORD=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "alld = []\n",
    "allg = []\n",
    "for dg in TRAIN_DATA_WORD[0:1000]:\n",
    "    if dg[1][\"entities\"] :\n",
    "        for d  in dg[1][\"entities\"]:\n",
    "            if d[2] == \"DISEASE\":\n",
    "                allg.append(dg[0][d[0]:d[1]])\n",
    "            if d[2] == \"GENE\":\n",
    "#                 print()\n",
    "#                 print(dg[0][d[0]:d[1]], d[2])\n",
    "#                 if dg[0][d[0]:d[1]] == \"set\":\n",
    "#                     print(dg)\n",
    "                alld.append(dg[0][d[0]:d[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(allg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16458/16458 [14:05<00:00, 19.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_DATA_WORD=[]\n",
    "test_i = 0\n",
    "nb = 0\n",
    "for line in tqdm(test):\n",
    "    text = json.loads(line)[\"ab\"]\n",
    "    test_i += 1\n",
    "    \n",
    "    detected_disease = []\n",
    "    for maladie in maladies:\n",
    "        if maladie in text:\n",
    "            detected_disease.append(maladie.lower())\n",
    "    \n",
    "    for sentence in tokenize.sent_tokenize(text):\n",
    "        if len(sentence) > 10:\n",
    "            uhm = {\"entities\": []}\n",
    "            \n",
    "            for detect in detected_disease:\n",
    "                if detect in sentence.lower():\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(detect), sentence.lower())]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(detect), \"DISEASE\")  )\n",
    "            \n",
    "            for n in tokenize.word_tokenize(sentence):\n",
    "                if n.lower() in genes_clean:\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n.lower()), sentence.lower())]\n",
    "                    for qs in pmz:\n",
    "                        uhm[\"entities\"].append( (qs, qs+len(n), \"GENE\")  )\n",
    "            \n",
    "            TEST_DATA_WORD.append( (sentence, uhm) )\n",
    "    \n",
    "    if test_i % 2000 == 0:\n",
    "        with open(\"test/normal/test_set_\"+str(nb)+\".json\", \"w\") as t:\n",
    "            t.write(json.dumps(TEST_DATA_WORD))\n",
    "        nb += 1\n",
    "        TEST_DATA_WORD=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train set from NCBItestset_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46368d58e7f64cf29a14d5bf877b8304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6924), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"NCBItrainset_corpus.txt\") as g:\n",
    "    NCBItest = g.readlines()\n",
    "\n",
    "TEST_NCB = []\n",
    "art = [\"\", {\"entities\":[]}]\n",
    "for line in tqdm_notebook(NCBItest):\n",
    "    \n",
    "    if \"|a|\" in line:\n",
    "        text = line.split(\"|a|\")[1]\n",
    "        art[0] = text.decode(\"utf-8\")\n",
    "        \n",
    "        for n in tokenize.word_tokenize(text):\n",
    "                if n.lower() in genes_clean:\n",
    "                    pmz = [m.start() for m in re.finditer(r'\\b%s\\b' % re.escape(n.lower()), text.lower())]\n",
    "                    for qs in pmz:\n",
    "                        art[1][\"entities\"].append( (qs, qs+len(n), \"GENE\")  )\n",
    "    \n",
    "    if \"Modifier\" in line or \"SpecificDisease\" in line or \"DiseaseClass\" in line:\n",
    "        d = tokenize.word_tokenize(line)[:-1]\n",
    "        entity = [int(d[1]), int(d[2]), \"DISEASE\"]\n",
    "        art[1][\"entities\"].append(entity)\n",
    "    if line == '\\n':\n",
    "        TEST_NCB.append(tuple(art))\n",
    "        art = [\"\", {\"entities\":[]}]\n",
    "\n",
    "TEST_NCB = TEST_NCB[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = []\n",
    "files = glob.glob(\"train/normal/*.json\")\n",
    "for f in files:\n",
    "    with open(f) as fl:\n",
    "        js = json.load(fl)\n",
    "        for j in js:\n",
    "            TRAIN_DATA.append(tuple(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_SAMPLE = shuffle(TRAIN_DATA)[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NCBI = shuffle(TRAIN_DATA_SAMPLE + TEST_NCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train/normal_NCBI/train_set_0.json\", \"w\") as t:\n",
    "            t.write(json.dumps(TRAIN_NCBI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train/normal_NCBI/train_set_NCBI_only.json\", \"w\") as t:\n",
    "            t.write(json.dumps(TEST_NCB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = open(\"../autism.json/autism.json\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"test/sample_autism.json\", \"w\") as t:\n",
    "#     t.write(json.dumps(test[0:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"test/sample_autism.json\", \"r\") as t:\n",
    "#     ok = json.load(t)\n",
    "#     article = json.loads(ok[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
